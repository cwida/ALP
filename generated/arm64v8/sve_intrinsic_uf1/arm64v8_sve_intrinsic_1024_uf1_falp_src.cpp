#include "alp/alp.hpp"
#include "alp/macros.hpp"
#ifdef __ARM_FEATURE_SVE
#include <arm_sve.h>
#else
#include "farm_sve.h"
#endif /* __ARM_FEATURE_SVE */ 
namespace generated
{
	namespace falp::arm64v8
	{
		namespace sve
		{
			static void falp_0bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), base_0);
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), base_0);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_1bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_2bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_3bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_4bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_5bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_6bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_7bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_8bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_9bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_10bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_11bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_12bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_13bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_14bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_15bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_16bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_17bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,15), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 17) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_18bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_19bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 17) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,15), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 17) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,17), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 19) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_20bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_21bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 19) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 17) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,15), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 17) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,17), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 19) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,19), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 21) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_22bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_23bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 21) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,21), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 17) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 19) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,19), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 19) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 17) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,17), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 21) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,15), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 23) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_24bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_25bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 17) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,17), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 19) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 23) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,23), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,15), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 21) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 21) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,21), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 23) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 19) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,19), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 17) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 25) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_26bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_27bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 17) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 23) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,23), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 21) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 19) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,19), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 25) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,15), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 25) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,25), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 19) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 21) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,21), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 23) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 17) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,17), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 27) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_28bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_29bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 23) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 17) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 19) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,19), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 25) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,25), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 27) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 21) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,15), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 21) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,21), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 27) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,27), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 25) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 19) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 17) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,17), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 23) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,23), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 29) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_30bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_31bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 29) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 27) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 25) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 23) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 21) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 19) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 17) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,15), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 17) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,17), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 19) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,19), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 21) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,21), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 23) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,23), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 25) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,25), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 27) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,27), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 29) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,29), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 31) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_32bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_33bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 31) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,31), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 29) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,29), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 27) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,27), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 25) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,25), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 23) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,23), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 21) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,21), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 19) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,19), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 17) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,17), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,15), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 17) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 19) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 21) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 23) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 25) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 27) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 29) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 31) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 33) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_34bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_35bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 29) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,29), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 23) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,23), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 17) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,17), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 19) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 25) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 31) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 33) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,33), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 27) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,27), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 21) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,21), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,15), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 21) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 27) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 33) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 31) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,31), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 25) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,25), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 19) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,19), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 17) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 23) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 29) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 35) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_36bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_37bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 27) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,27), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 17) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,17), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 23) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 33) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 31) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,31), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 21) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,21), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 19) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 29) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 35) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,35), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 25) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,25), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,15), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 25) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 35) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 29) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,29), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 19) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,19), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 21) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 31) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 33) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,33), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 23) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,23), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 17) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 27) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 37) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_38bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_39bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 25) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,25), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 17) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 31) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 33) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,33), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 19) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,19), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 23) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 37) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 27) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,27), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 29) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 35) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,35), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 21) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,21), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 21) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 35) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 29) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,29), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,15), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 27) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 37) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,37), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 23) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,23), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 19) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 33) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 31) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,31), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 17) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,17), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 25) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 39) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_40bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_41bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 41) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 23) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,23), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 41) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 41) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 31) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 33) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,33), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 41) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,15), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 41) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 21) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 41) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 39) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 25) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,25), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 41) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 41) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 29) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 35) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,35), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 41) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 17) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,17), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 41) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 19) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 41) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 37) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 27) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,27), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 41) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 41) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 27) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 37) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,37), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 41) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 19) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,19), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 41) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 17) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 41) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 35) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 29) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,29), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 41) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 41) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 25) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 39) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,39), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 41) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 21) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,21), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 41) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 41) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 33) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 31) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,31), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 41) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 41) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 640);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 23) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 41) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_42bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 42) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 42) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 42) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 42) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 42) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 42) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 42) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 42) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 42) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 42) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 42) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 42) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 42) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 42) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 42) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 42) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 42) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 42) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 42) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 42) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 42) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 42) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 640);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 42) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 656);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 42) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_43bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 43) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 21) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,21), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 42) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,42), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 43) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 23) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 41) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,41), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 43) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 19) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,19), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 43) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 25) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 39) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,39), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 43) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 17) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,17), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 43) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 27) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 37) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,37), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 43) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,15), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 43) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 29) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 35) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,35), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 43) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 43) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 31) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 33) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,33), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 43) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 43) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 33) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 31) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,31), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 43) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 43) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 35) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 29) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,29), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 43) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 43) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 37) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 27) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,27), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 43) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 17) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 43) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 39) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 25) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,25), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 43) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 19) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 43) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 41) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 23) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 640);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,23), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 43) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 656);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 42) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 672);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 21) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 43) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_44bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 44) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 44) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 44) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 44) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 44) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 44) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 44) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 44) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 44) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 44) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 44) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 44) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 44) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 44) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 44) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 640);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 656);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 44) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 672);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 688);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_45bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 45) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 19) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,19), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 45) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 33) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 31) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,31), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 45) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 21) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 43) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,43), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 45) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 17) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,17), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 45) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 35) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 29) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,29), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 45) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 42) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 23) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 41) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,41), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 45) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,15), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 45) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 37) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 27) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,27), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 45) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 25) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 39) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,39), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 45) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 45) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 39) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 25) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,25), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 45) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 27) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 37) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,37), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 45) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 45) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 41) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 23) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,23), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 42) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,42), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 45) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 29) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 35) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,35), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 45) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 17) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 45) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 43) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 21) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,21), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 640);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 45) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 656);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 31) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 33) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 672);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,33), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 45) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 688);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 704);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 19) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 45) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_46bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 46) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 46) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 46) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 46) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 42) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,42), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 46) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 46) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 42) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 46) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 46) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 46) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 46) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 46) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 46) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 46) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 46) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 42) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,42), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 46) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 46) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 42) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 46) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 640);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 46) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 656);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 672);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 688);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 46) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 704);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 720);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 46) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_47bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 47) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 17) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,17), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 47) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 43) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 21) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,21), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 47) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 39) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 25) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,25), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 42) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,42), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 47) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 35) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 29) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,29), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 46) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,46), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 47) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 31) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 33) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,33), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 47) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 27) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 37) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,37), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 47) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 23) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 41) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,41), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 47) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 19) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 45) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,45), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 47) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,15), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 47) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 45) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 19) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,19), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 47) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 41) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 23) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,23), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 47) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 37) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 27) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,27), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 47) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 33) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 31) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,31), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 47) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 46) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 29) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 35) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,35), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 47) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 42) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 640);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 25) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 39) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 656);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,39), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 47) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 672);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 688);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 21) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 43) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 704);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,43), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 47) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 720);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 736);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 17) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 47) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_48bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 640);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 656);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 672);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 688);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 704);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 720);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 736);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 752);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_49bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 49) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,15), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 19) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 45) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,45), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 49) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 23) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 41) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,41), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 49) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 42) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 27) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 37) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,37), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 49) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 46) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 31) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 33) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,33), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 49) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 35) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 29) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,29), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 49) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 39) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 25) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,25), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 49) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 43) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 21) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,21), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 49) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 47) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 17) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,17), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 17) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 47) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,47), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 49) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 21) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 43) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,43), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 49) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 25) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 39) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,39), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 49) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 29) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 35) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,35), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 49) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 33) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 31) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,31), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 46) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,46), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 49) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 640);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 37) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 27) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 656);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,27), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 42) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 672);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,42), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 49) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 688);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 41) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 23) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 704);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,23), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 720);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 49) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 736);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 45) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 19) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 752);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,19), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 768);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 49) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_50bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 50) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 42) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,42), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 50) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 50) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 50) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 46) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 46) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,46), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 50) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 50) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 50) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 42) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 50) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 50) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 42) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,42), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 50) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 50) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 50) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 46) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 46) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,46), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 50) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 640);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 656);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 672);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 50) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 688);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 704);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 720);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 736);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 50) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 752);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 42) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 768);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 784);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 50) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_51bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 51) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 25) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 39) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,39), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 51) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 50) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 37) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 27) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,27), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 51) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 49) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,15), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 23) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 41) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,41), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 51) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 35) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 29) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,29), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 42) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,42), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 51) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 47) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 17) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,17), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 21) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 43) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,43), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 51) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 46) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 33) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 31) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,31), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 51) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 45) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 19) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,19), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 19) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 45) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,45), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 51) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 31) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 33) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,33), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 46) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,46), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 51) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 43) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 21) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,21), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 17) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 47) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,47), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 51) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 42) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 29) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 35) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,35), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 51) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 640);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 41) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 23) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 656);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,23), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 672);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 49) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 688);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,49), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 51) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 704);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 720);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 27) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 37) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 736);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,37), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 50) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 752);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,50), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 51) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 768);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 39) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 25) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 784);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,25), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 800);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 51) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_52bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 52) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 52) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 52) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 52) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 52) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 52) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 52) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 52) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 52) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 52) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 52) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 52) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 52) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 640);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 656);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 672);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 688);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 52) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 704);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 720);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 736);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 752);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 52) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 768);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 784);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 800);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 816);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 52) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_53bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 53) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 42) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 31) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 33) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,33), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 53) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 51) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 29) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 35) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,35), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 46) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,46), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 53) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 49) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,15), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 27) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 37) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,37), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 53) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 47) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 17) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,17), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 25) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 39) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,39), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 50) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,50), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 53) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 45) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 19) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,19), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 23) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 41) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,41), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 52) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,52), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 53) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 43) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 21) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,21), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 21) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 43) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,43), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 53) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 52) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 41) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 23) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,23), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 19) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 45) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,45), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 53) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 50) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 39) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 25) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,25), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 17) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 47) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,47), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 53) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 640);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 37) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 27) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 656);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,27), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 672);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 49) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 688);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,49), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 53) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 704);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 46) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 720);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 35) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 29) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 736);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,29), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 752);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 51) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 768);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,51), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 53) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 784);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 800);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 33) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 31) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 816);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,31), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 42) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 832);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,42), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 53) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_54bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 54) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 50) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,50), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 54) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 46) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,46), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 54) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 52) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 42) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 42) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,42), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 52) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,52), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 54) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 46) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 54) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 50) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 54) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 54) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 50) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,50), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 54) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 46) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,46), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 54) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 52) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 42) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 640);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 656);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 42) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 672);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,42), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 52) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 688);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,52), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 54) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 704);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 46) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 720);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 736);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 752);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 768);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 54) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 784);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 50) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 800);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 816);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 832);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 848);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 54) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_55bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 55) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 46) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 37) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 27) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,27), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 19) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 45) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,45), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 54) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,54), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 55) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 47) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 17) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,17), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 29) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 35) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,35), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 53) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,53), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 55) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 39) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 25) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,25), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 21) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 43) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,43), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 52) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,52), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 55) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 49) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,15), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 31) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 33) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,33), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 42) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,42), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 51) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,51), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 55) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 50) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 41) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 23) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,23), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 23) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 41) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,41), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 50) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,50), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 55) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 51) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 42) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 33) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 31) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,31), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 49) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,49), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 55) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 52) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 43) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 21) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,21), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 640);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 25) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 39) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 656);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,39), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 672);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 55) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 688);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 53) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 704);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 720);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 35) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 29) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 736);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,29), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 752);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 17) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 47) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 768);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,47), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 55) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 784);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 54) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 800);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 45) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 19) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 816);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,19), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 832);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 27) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 37) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 848);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,37), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 46) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 864);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,46), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 55) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_56bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 56) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 56) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 56) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 56) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 56) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 56) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 56) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 56) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 56) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 56) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 56) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 640);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 656);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 56) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 672);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 56) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 688);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 704);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 720);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 736);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 752);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 768);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 56) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 784);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 56) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 800);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 816);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 832);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 848);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 864);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 880);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 56) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_57bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 57) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 50) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 43) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 21) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,21), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 29) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 35) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,35), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 42) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1)) ,42), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 49) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,49), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 56) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,56), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 57) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 51) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 37) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 27) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,27), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 23) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 41) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,41), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 55) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,55), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 57) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 52) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 45) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 19) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,19), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 31) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 33) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,33), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 17) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 47) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,47), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 54) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,54), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 57) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 53) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 46) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 39) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 25) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,25), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 25) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 39) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,39), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 46) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,46), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 53) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,53), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 57) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 54) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 47) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 17) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,17), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 33) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 31) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,31), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 19) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 45) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,45), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 52) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 640);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,52), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 57) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 656);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 55) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 672);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 688);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 41) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 23) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 704);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,23), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 720);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 27) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 37) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 736);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,37), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 752);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 51) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 768);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,51), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 57) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 784);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 56) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 800);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 49) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 816);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 42) - 1)) ,15), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 832);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 35) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 29) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 848);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,29), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 864);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 21) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 43) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 880);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,43), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 50) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 896);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,50), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 57) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_58bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 58) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 52) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 46) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 42) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,42), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 54) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,54), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 58) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 56) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 50) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 50) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,50), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 56) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,56), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 58) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 54) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 42) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 46) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,46), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 52) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,52), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 58) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 58) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 52) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 46) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 42) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,42), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 54) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,54), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 58) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 56) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 640);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 50) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 656);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 672);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 688);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 704);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 720);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 736);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 50) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 752);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,50), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 56) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 768);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,56), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 58) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 784);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 54) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 800);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 816);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 42) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 832);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 848);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 864);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 880);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 46) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 896);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,46), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 52) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 912);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,52), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 58) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_59bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 59) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 54) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 49) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,15), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 39) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 25) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,25), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 29) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 35) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,35), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 19) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 45) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,45), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 50) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,50), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 55) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,55), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 59) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 58) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 53) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 43) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 21) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,21), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 33) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 31) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,31), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 23) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 41) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,41), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 46) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,46), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 51) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,51), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 56) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,56), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 59) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 57) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 52) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 47) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 17) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 42) - 1)) ,17), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 37) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 27) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,27), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 27) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 37) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,37), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 42) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 17) - 1)) ,42), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 47) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,47), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 52) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,52), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 57) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,57), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 59) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 56) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 51) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 46) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 41) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 23) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 640);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,23), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 656);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 31) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 33) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 672);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,33), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 688);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 21) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 43) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 704);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,43), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 720);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 53) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 736);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,53), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 58) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 752);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,58), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 59) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 768);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 55) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 784);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 50) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 800);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 45) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 19) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 816);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,19), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 832);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 35) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 29) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 848);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,29), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 864);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 25) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 39) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 880);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,39), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 896);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 49) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 912);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,49), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 54) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 928);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,54), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 59) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_60bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 60) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 56) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 52) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 52) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,52), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 56) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,56), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 60) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 60) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 56) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 52) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 52) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,52), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 56) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,56), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 60) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 60) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 56) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 52) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 640);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 656);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 672);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 52) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 688);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,52), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 56) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 704);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,56), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 60) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 720);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 60) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 736);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 56) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 752);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 52) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 768);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 784);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 800);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 816);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 832);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 848);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 864);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 880);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 896);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 912);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 52) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 928);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,52), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 56) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 944);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,56), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 60) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_61bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 61) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 58) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 55) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 52) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 49) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 46) - 1)) ,15), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 43) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 21) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,21), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 37) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 27) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,27), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 31) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 33) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,33), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 25) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 39) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,39), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 42) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 19) - 1)) ,42), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 45) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,45), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 51) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,51), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 54) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,54), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 57) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,57), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 60) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,60), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 61) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 59) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 56) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 53) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 50) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 47) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 17) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,17), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 41) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 23) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,23), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 35) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 29) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,29), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 29) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 35) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,35), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 23) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 41) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,41), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 17) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 47) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,47), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 50) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,50), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 53) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,53), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 56) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,56), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 59) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 640);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,59), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 61) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 656);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 60) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 672);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 57) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 688);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 54) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 704);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 51) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 720);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 736);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 45) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 19) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 752);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 42) - 1)) ,19), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 768);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 39) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 25) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 784);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,25), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 800);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 33) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 31) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 816);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,31), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 832);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 27) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 37) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 848);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,37), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 864);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 21) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 43) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 880);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,43), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 46) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 896);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1)) ,46), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 49) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 912);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,49), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 52) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 928);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,52), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 55) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 944);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,55), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 58) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 960);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,58), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 61) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_62bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 62) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 60) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 58) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 56) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 54) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 52) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 50) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 46) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 42) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 42) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,42), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 46) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,46), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 50) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,50), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 52) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,52), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 54) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,54), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 56) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,56), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 58) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,58), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 60) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,60), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 62) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 62) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 60) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 58) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 56) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 54) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 52) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 50) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 46) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 640);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 656);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 42) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 672);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 688);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 704);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 720);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 736);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 752);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 768);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 784);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 800);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 816);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 42) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 832);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,42), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 848);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 46) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 864);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,46), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 880);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 50) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 896);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,50), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 52) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 912);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,52), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 54) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 928);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,54), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 56) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 944);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,56), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 58) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 960);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,58), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 60) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 976);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,60), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 62) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_63bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					tmp_0 = svand_u64_x(pg, register_0, svdup_u64((1ULL << 63) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 63), svdup_u64((1ULL << 1) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 62) - 1)) ,1), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 62), svdup_u64((1ULL << 2) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 61) - 1)) ,2), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 61), svdup_u64((1ULL << 3) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 60) - 1)) ,3), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 60), svdup_u64((1ULL << 4) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 59) - 1)) ,4), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 59), svdup_u64((1ULL << 5) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 58) - 1)) ,5), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 58), svdup_u64((1ULL << 6) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 57) - 1)) ,6), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 57), svdup_u64((1ULL << 7) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 56) - 1)) ,7), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 56), svdup_u64((1ULL << 8) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 55) - 1)) ,8), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 55), svdup_u64((1ULL << 9) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 54) - 1)) ,9), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 54), svdup_u64((1ULL << 10) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 53) - 1)) ,10), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 53), svdup_u64((1ULL << 11) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 52) - 1)) ,11), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 52), svdup_u64((1ULL << 12) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 51) - 1)) ,12), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 51), svdup_u64((1ULL << 13) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 50) - 1)) ,13), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 50), svdup_u64((1ULL << 14) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 49) - 1)) ,14), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 49), svdup_u64((1ULL << 15) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 48) - 1)) ,15), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 48), svdup_u64((1ULL << 16) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 47) - 1)) ,16), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 47), svdup_u64((1ULL << 17) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 46) - 1)) ,17), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 46), svdup_u64((1ULL << 18) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 45) - 1)) ,18), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 45), svdup_u64((1ULL << 19) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 44) - 1)) ,19), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 44), svdup_u64((1ULL << 20) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 43) - 1)) ,20), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 43), svdup_u64((1ULL << 21) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 42) - 1)) ,21), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 42), svdup_u64((1ULL << 22) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 41) - 1)) ,22), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 41), svdup_u64((1ULL << 23) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 40) - 1)) ,23), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 40), svdup_u64((1ULL << 24) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 39) - 1)) ,24), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 39), svdup_u64((1ULL << 25) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 38) - 1)) ,25), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 38), svdup_u64((1ULL << 26) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 37) - 1)) ,26), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 37), svdup_u64((1ULL << 27) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 36) - 1)) ,27), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 36), svdup_u64((1ULL << 28) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 35) - 1)) ,28), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 35), svdup_u64((1ULL << 29) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 34) - 1)) ,29), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 34), svdup_u64((1ULL << 30) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 33) - 1)) ,30), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 33), svdup_u64((1ULL << 31) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 32) - 1)) ,31), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 32), svdup_u64((1ULL << 32) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 31) - 1)) ,32), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 31), svdup_u64((1ULL << 33) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 30) - 1)) ,33), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 30), svdup_u64((1ULL << 34) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 29) - 1)) ,34), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 29), svdup_u64((1ULL << 35) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 28) - 1)) ,35), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 28), svdup_u64((1ULL << 36) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 27) - 1)) ,36), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 27), svdup_u64((1ULL << 37) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 26) - 1)) ,37), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 26), svdup_u64((1ULL << 38) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 25) - 1)) ,38), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 25), svdup_u64((1ULL << 39) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 24) - 1)) ,39), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 24), svdup_u64((1ULL << 40) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 640);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 23) - 1)) ,40), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 23), svdup_u64((1ULL << 41) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 656);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 22) - 1)) ,41), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 22), svdup_u64((1ULL << 42) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 672);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 21) - 1)) ,42), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 21), svdup_u64((1ULL << 43) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 688);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 20) - 1)) ,43), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 20), svdup_u64((1ULL << 44) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 704);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 19) - 1)) ,44), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 19), svdup_u64((1ULL << 45) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 720);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 18) - 1)) ,45), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 18), svdup_u64((1ULL << 46) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 736);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 17) - 1)) ,46), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 17), svdup_u64((1ULL << 47) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 752);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 16) - 1)) ,47), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 16), svdup_u64((1ULL << 48) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 768);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 15) - 1)) ,48), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 15), svdup_u64((1ULL << 49) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 784);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 14) - 1)) ,49), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 14), svdup_u64((1ULL << 50) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 800);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 13) - 1)) ,50), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 13), svdup_u64((1ULL << 51) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 816);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 12) - 1)) ,51), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 12), svdup_u64((1ULL << 52) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 832);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 11) - 1)) ,52), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 11), svdup_u64((1ULL << 53) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 848);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 10) - 1)) ,53), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 10), svdup_u64((1ULL << 54) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 864);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 9) - 1)) ,54), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 9), svdup_u64((1ULL << 55) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 880);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 8) - 1)) ,55), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 8), svdup_u64((1ULL << 56) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 896);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 7) - 1)) ,56), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 7), svdup_u64((1ULL << 57) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 912);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 6) - 1)) ,57), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 6), svdup_u64((1ULL << 58) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 928);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 5) - 1)) ,58), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 5), svdup_u64((1ULL << 59) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 944);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 4) - 1)) ,59), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 4), svdup_u64((1ULL << 60) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 960);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 3) - 1)) ,60), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 3), svdup_u64((1ULL << 61) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 976);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 2) - 1)) ,61), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 2), svdup_u64((1ULL << 62) - 1));
					register_0 = svld1(pg, in + (0 * 16) + (i) + 992);
					tmp_0 = svorr_u64_x(pg, svlsl_x(pg, svand_u64_x(pg, register_0, svdup_u64((1ULL << 1) - 1)) ,62), tmp_0);
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), tmp_dbl);
					tmp_0 = svand_u64_x(pg, svlsr_x(pg, register_0, 1), svdup_u64((1ULL << 63) - 1));
					tmp_0 += base_0;
					tmp_0 *= factor;
					FIX
					tmp_dbl *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), tmp_dbl);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			static void falp_64bw_64ow_128crw_1uf(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				[[maybe_unused]] auto out = (a_out_p);
				[[maybe_unused]] const auto in = (a_in_p);
				[[maybe_unused]] svuint64_t register_0;
				[[maybe_unused]] svuint64_t tmp_0;
				svbool_t pg = svwhilelt_b64(static_cast<int64_t>(0LL), (1024LL / 64));
				int64_t i = 0;
				[[maybe_unused]] svuint64_t base_0;
				do
				{
					register_0 = svld1(pg, in + (0 * 16) + (i) + 0);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 0), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 16);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 1), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 32);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 2), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 48);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 3), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 64);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 4), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 80);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 5), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 96);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 6), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 112);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 7), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 128);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 8), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 144);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 9), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 160);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 10), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 176);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 11), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 192);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 12), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 208);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 13), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 224);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 14), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 240);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 15), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 256);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 16), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 272);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 17), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 288);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 18), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 304);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 19), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 320);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 20), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 336);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 21), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 352);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 22), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 368);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 23), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 384);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 24), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 400);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 25), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 416);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 26), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 432);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 27), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 448);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 28), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 464);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 29), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 480);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 30), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 496);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 31), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 512);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 32), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 528);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 33), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 544);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 34), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 560);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 35), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 576);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 36), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 592);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 37), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 608);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 38), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 624);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 39), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 640);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 40), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 656);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 41), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 672);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 42), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 688);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 43), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 704);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 44), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 720);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 45), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 736);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 46), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 752);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 47), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 768);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 48), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 784);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 49), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 800);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 50), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 816);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 51), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 832);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 52), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 848);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 53), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 864);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 54), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 880);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 55), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 896);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 56), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 912);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 57), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 928);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 58), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 944);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 59), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 960);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 60), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 976);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 61), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 992);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 62), register_0);
					register_0 = svld1(pg, in + (0 * 16) + (i) + 1008);
					register_0 += base_0;
					register_0 *= factor;
					FIX
					register_0 *= frac10;
					svst1(pg, out + (i * 2) + (0 * 16 * 2) + (16 * 63), register_0);
					i += svcntd();
					pg = svwhilelt_b64(i, (1024LL / 64));
				}
				while (svptest_any(svptrue_b64(), pg));
			}
			void falp(const uint64_t *__restrict a_in_p, double *__restrict a_out_p, uint8_t bw, const uint64_t *__restrict a_base_p, uint8_t fac, uint8_t exp)
			{
				 switch (bw)
				{
					case 0:
					   falp_0bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 1:
					   falp_1bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 2:
					   falp_2bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 3:
					   falp_3bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 4:
					   falp_4bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 5:
					   falp_5bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 6:
					   falp_6bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 7:
					   falp_7bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 8:
					   falp_8bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 9:
					   falp_9bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 10:
					   falp_10bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 11:
					   falp_11bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 12:
					   falp_12bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 13:
					   falp_13bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 14:
					   falp_14bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 15:
					   falp_15bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 16:
					   falp_16bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 17:
					   falp_17bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 18:
					   falp_18bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 19:
					   falp_19bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 20:
					   falp_20bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 21:
					   falp_21bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 22:
					   falp_22bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 23:
					   falp_23bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 24:
					   falp_24bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 25:
					   falp_25bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 26:
					   falp_26bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 27:
					   falp_27bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 28:
					   falp_28bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 29:
					   falp_29bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 30:
					   falp_30bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 31:
					   falp_31bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 32:
					   falp_32bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 33:
					   falp_33bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 34:
					   falp_34bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 35:
					   falp_35bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 36:
					   falp_36bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 37:
					   falp_37bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 38:
					   falp_38bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 39:
					   falp_39bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 40:
					   falp_40bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 41:
					   falp_41bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 42:
					   falp_42bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 43:
					   falp_43bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 44:
					   falp_44bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 45:
					   falp_45bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 46:
					   falp_46bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 47:
					   falp_47bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 48:
					   falp_48bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 49:
					   falp_49bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 50:
					   falp_50bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 51:
					   falp_51bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 52:
					   falp_52bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 53:
					   falp_53bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 54:
					   falp_54bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 55:
					   falp_55bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 56:
					   falp_56bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 57:
					   falp_57bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 58:
					   falp_58bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 59:
					   falp_59bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 60:
					   falp_60bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 61:
					   falp_61bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 62:
					   falp_62bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 63:
					   falp_63bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
					case 64:
					   falp_64bw_64ow_128crw_1uf(a_in_p, a_out_p, a_base_p, fac, exp);
					   break;
				}
			}
		}
	}
}
;
